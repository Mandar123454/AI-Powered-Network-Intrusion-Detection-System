{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260a736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.feature_selection import VarianceThreshold, RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6792ae",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab086d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from exploration notebook\n",
    "DATA_DIR = Path('../data')\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Try to load CICIDS2017 or use sample data\n",
    "def load_or_create_data(n_samples=20000):\n",
    "    \"\"\"Load dataset or create sample data.\"\"\"\n",
    "    csv_files = list((DATA_DIR / 'datasets' / 'CICIDS2017').glob('*.csv'))\n",
    "    \n",
    "    if csv_files:\n",
    "        print(f\"Loading {csv_files[0].name}...\")\n",
    "        df = pd.read_csv(csv_files[0], nrows=n_samples)\n",
    "        return df\n",
    "    \n",
    "    # Create sample data\n",
    "    print(\"Creating sample data for demonstration...\")\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    labels = ['BENIGN'] * int(n_samples * 0.7) + \\\n",
    "             ['DoS Hulk'] * int(n_samples * 0.1) + \\\n",
    "             ['PortScan'] * int(n_samples * 0.08) + \\\n",
    "             ['DDoS'] * int(n_samples * 0.05) + \\\n",
    "             ['Bot'] * int(n_samples * 0.04) + \\\n",
    "             ['Brute Force'] * int(n_samples * 0.03)\n",
    "    \n",
    "    n = len(labels)\n",
    "    \n",
    "    data = {\n",
    "        'Flow Duration': np.abs(np.random.exponential(1000000, n)),\n",
    "        'Total Fwd Packets': np.random.poisson(10, n),\n",
    "        'Total Backward Packets': np.random.poisson(8, n),\n",
    "        'Total Length of Fwd Packets': np.abs(np.random.exponential(500, n)),\n",
    "        'Total Length of Bwd Packets': np.abs(np.random.exponential(400, n)),\n",
    "        'Fwd Packet Length Max': np.random.randint(0, 1500, n),\n",
    "        'Fwd Packet Length Min': np.random.randint(0, 100, n),\n",
    "        'Fwd Packet Length Mean': np.abs(np.random.exponential(200, n)),\n",
    "        'Fwd Packet Length Std': np.abs(np.random.exponential(100, n)),\n",
    "        'Bwd Packet Length Max': np.random.randint(0, 1500, n),\n",
    "        'Bwd Packet Length Min': np.random.randint(0, 100, n),\n",
    "        'Bwd Packet Length Mean': np.abs(np.random.exponential(180, n)),\n",
    "        'Bwd Packet Length Std': np.abs(np.random.exponential(90, n)),\n",
    "        'Flow Bytes/s': np.abs(np.random.exponential(10000, n)),\n",
    "        'Flow Packets/s': np.abs(np.random.exponential(100, n)),\n",
    "        'Flow IAT Mean': np.abs(np.random.exponential(50000, n)),\n",
    "        'Flow IAT Std': np.abs(np.random.exponential(30000, n)),\n",
    "        'Flow IAT Max': np.abs(np.random.exponential(100000, n)),\n",
    "        'Flow IAT Min': np.abs(np.random.exponential(1000, n)),\n",
    "        'Fwd IAT Total': np.abs(np.random.exponential(500000, n)),\n",
    "        'Fwd IAT Mean': np.abs(np.random.exponential(50000, n)),\n",
    "        'Fwd IAT Std': np.abs(np.random.exponential(30000, n)),\n",
    "        'Bwd IAT Total': np.abs(np.random.exponential(400000, n)),\n",
    "        'Bwd IAT Mean': np.abs(np.random.exponential(40000, n)),\n",
    "        'Bwd IAT Std': np.abs(np.random.exponential(25000, n)),\n",
    "        'Fwd PSH Flags': np.random.binomial(5, 0.3, n),\n",
    "        'Bwd PSH Flags': np.random.binomial(3, 0.2, n),\n",
    "        'Fwd Header Length': np.random.randint(20, 100, n),\n",
    "        'Bwd Header Length': np.random.randint(20, 80, n),\n",
    "        'Fwd Packets/s': np.abs(np.random.exponential(50, n)),\n",
    "        'Bwd Packets/s': np.abs(np.random.exponential(40, n)),\n",
    "        'Min Packet Length': np.random.randint(0, 100, n),\n",
    "        'Max Packet Length': np.random.randint(100, 1500, n),\n",
    "        'Packet Length Mean': np.abs(np.random.exponential(150, n)),\n",
    "        'Packet Length Std': np.abs(np.random.exponential(80, n)),\n",
    "        'Packet Length Variance': np.abs(np.random.exponential(5000, n)),\n",
    "        'SYN Flag Count': np.random.binomial(3, 0.4, n),\n",
    "        'ACK Flag Count': np.random.binomial(10, 0.6, n),\n",
    "        'FIN Flag Count': np.random.binomial(2, 0.3, n),\n",
    "        'RST Flag Count': np.random.binomial(1, 0.1, n),\n",
    "        'PSH Flag Count': np.random.binomial(3, 0.3, n),\n",
    "        'URG Flag Count': np.random.binomial(1, 0.05, n),\n",
    "        'Down/Up Ratio': np.random.uniform(0, 5, n),\n",
    "        'Average Packet Size': np.abs(np.random.exponential(200, n)),\n",
    "        'Init_Win_bytes_forward': np.random.randint(0, 65535, n),\n",
    "        'Init_Win_bytes_backward': np.random.randint(0, 65535, n),\n",
    "        'Destination Port': np.random.choice([80, 443, 22, 21, 8080, 3389, 53], n),\n",
    "        'Protocol': np.random.choice([6, 17, 1], n, p=[0.7, 0.25, 0.05]),\n",
    "        'Label': labels\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df = load_or_create_data()\n",
    "print(f\"\\nLoaded {len(df)} samples with {len(df.columns)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a630f772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic info\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nColumn types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83a5b33",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c96940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Find label column\n",
    "label_col = 'Label' if 'Label' in df.columns else 'label'\n",
    "print(f\"Label column: {label_col}\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=[label_col])\n",
    "y = df[label_col]\n",
    "\n",
    "print(f\"Features: {X.shape}\")\n",
    "print(f\"Target: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab73e18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "print(\"Missing values before cleaning:\")\n",
    "missing_before = X.isnull().sum().sum()\n",
    "print(f\"  Total: {missing_before}\")\n",
    "\n",
    "# Fill missing with median for numeric columns\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "X[numeric_cols] = X[numeric_cols].fillna(X[numeric_cols].median())\n",
    "\n",
    "print(f\"\\nMissing values after cleaning: {X.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eb8dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle infinite values\n",
    "print(\"Infinite values before cleaning:\")\n",
    "inf_count = np.isinf(X[numeric_cols]).sum().sum()\n",
    "print(f\"  Total: {inf_count}\")\n",
    "\n",
    "# Replace infinites with large values\n",
    "X[numeric_cols] = X[numeric_cols].replace([np.inf, -np.inf], np.nan)\n",
    "X[numeric_cols] = X[numeric_cols].fillna(X[numeric_cols].median())\n",
    "\n",
    "print(f\"\\nInfinite values after cleaning: {np.isinf(X[numeric_cols]).sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7b7244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove constant features\n",
    "print(\"\\nRemoving constant features...\")\n",
    "variance_selector = VarianceThreshold(threshold=0)\n",
    "variance_selector.fit(X[numeric_cols])\n",
    "\n",
    "constant_features = numeric_cols[~variance_selector.get_support()].tolist()\n",
    "print(f\"Constant features found: {len(constant_features)}\")\n",
    "if constant_features:\n",
    "    print(f\"  {constant_features}\")\n",
    "    X = X.drop(columns=constant_features)\n",
    "\n",
    "print(f\"\\nFeatures after removing constants: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb8122f",
   "metadata": {},
   "source": [
    "## 3. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d118025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels for feature selection\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "print(\"Classes:\", le.classes_)\n",
    "print(\"Encoded values:\", np.unique(y_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba34972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance using Random Forest\n",
    "print(\"Calculating feature importance with Random Forest...\")\n",
    "\n",
    "# Get numeric features only\n",
    "X_numeric = X.select_dtypes(include=[np.number])\n",
    "\n",
    "# Quick RF for feature importance\n",
    "rf = RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_numeric, y_encoded)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_numeric.columns,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 Important Features:\")\n",
    "print(feature_importance.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9944da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(20)\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(top_features)))\n",
    "plt.barh(range(len(top_features)), top_features['importance'], color=colors)\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 20 Feature Importances (Random Forest)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(PROCESSED_DIR / 'feature_importance.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bebb218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top features based on importance\n",
    "TOP_K = 30\n",
    "selected_features = feature_importance.head(TOP_K)['feature'].tolist()\n",
    "\n",
    "print(f\"\\nSelected {TOP_K} features:\")\n",
    "for i, feat in enumerate(selected_features, 1):\n",
    "    print(f\"  {i}. {feat}\")\n",
    "\n",
    "X_selected = X_numeric[selected_features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88565af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove highly correlated features\n",
    "print(\"\\nRemoving highly correlated features (>0.95)...\")\n",
    "\n",
    "corr_matrix = X_selected.corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n",
    "print(f\"Highly correlated features to remove: {len(to_drop)}\")\n",
    "if to_drop:\n",
    "    print(f\"  {to_drop}\")\n",
    "    X_selected = X_selected.drop(columns=to_drop)\n",
    "\n",
    "print(f\"\\nFinal feature count: {X_selected.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9819c4ba",
   "metadata": {},
   "source": [
    "## 4. Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f473493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation to skewed features\n",
    "print(\"Applying log transformation to skewed features...\")\n",
    "\n",
    "skewness = X_selected.skew()\n",
    "skewed_features = skewness[skewness.abs() > 2].index.tolist()\n",
    "\n",
    "print(f\"Skewed features (|skew| > 2): {len(skewed_features)}\")\n",
    "\n",
    "X_transformed = X_selected.copy()\n",
    "for col in skewed_features:\n",
    "    if X_transformed[col].min() >= 0:\n",
    "        X_transformed[col] = np.log1p(X_transformed[col])\n",
    "\n",
    "# Compare skewness before and after\n",
    "print(\"\\nSkewness comparison (sample):\")\n",
    "for col in skewed_features[:5]:\n",
    "    before = X_selected[col].skew()\n",
    "    after = X_transformed[col].skew()\n",
    "    print(f\"  {col}: {before:.2f} -> {after:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909d4a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "print(\"\\nApplying StandardScaler...\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_transformed),\n",
    "    columns=X_transformed.columns,\n",
    "    index=X_transformed.index\n",
    ")\n",
    "\n",
    "print(\"\\nScaled data statistics:\")\n",
    "print(X_scaled.describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6103cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize scaled distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "sample_cols = X_scaled.columns[:6]\n",
    "for idx, col in enumerate(sample_cols):\n",
    "    axes[idx].hist(X_scaled[col], bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_xlabel(col[:30])\n",
    "    axes[idx].set_title(f'Scaled: {col[:25]}...')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(PROCESSED_DIR / 'scaled_distributions.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cfa532",
   "metadata": {},
   "source": [
    "## 5. Handle Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f273ae48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class distribution\n",
    "print(\"Class distribution:\")\n",
    "class_dist = pd.Series(y).value_counts()\n",
    "print(class_dist)\n",
    "\n",
    "# Calculate imbalance ratio\n",
    "imbalance_ratio = class_dist.max() / class_dist.min()\n",
    "print(f\"\\nImbalance ratio: {imbalance_ratio:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fe8241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data before applying SMOTE\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "print(\"\\nTraining class distribution:\")\n",
    "print(pd.Series(y_train).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5f09eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE for oversampling minority classes\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "    \n",
    "    print(\"Applying SMOTE + Undersampling...\")\n",
    "    \n",
    "    # Combined approach: oversample minority + undersample majority\n",
    "    over = SMOTE(sampling_strategy=0.5, random_state=42)\n",
    "    under = RandomUnderSampler(sampling_strategy=0.8, random_state=42)\n",
    "    \n",
    "    steps = [('over', over), ('under', under)]\n",
    "    pipeline = ImbPipeline(steps=steps)\n",
    "    \n",
    "    X_train_balanced, y_train_balanced = pipeline.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(f\"\\nBalanced training set: {X_train_balanced.shape}\")\n",
    "    print(\"\\nBalanced class distribution:\")\n",
    "    print(pd.Series(y_train_balanced).value_counts())\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"imbalanced-learn not installed. Using original data.\")\n",
    "    print(\"Install with: pip install imbalanced-learn\")\n",
    "    X_train_balanced = X_train\n",
    "    y_train_balanced = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cec32e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution before and after\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Before\n",
    "pd.Series(y_train).value_counts().plot(kind='bar', ax=axes[0], color='coral', edgecolor='black')\n",
    "axes[0].set_title('Before Balancing')\n",
    "axes[0].set_xlabel('Class')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# After\n",
    "pd.Series(y_train_balanced).value_counts().plot(kind='bar', ax=axes[1], color='seagreen', edgecolor='black')\n",
    "axes[1].set_title('After Balancing (SMOTE + Undersampling)')\n",
    "axes[1].set_xlabel('Class')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(PROCESSED_DIR / 'class_balance.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac1d7e8",
   "metadata": {},
   "source": [
    "## 6. Create Additional Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed28fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_additional_features(df):\n",
    "    \"\"\"\n",
    "    Create additional engineered features.\n",
    "    \"\"\"\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # Ratio features (if columns exist)\n",
    "    if 'Total Fwd Packets' in df.columns and 'Total Backward Packets' in df.columns:\n",
    "        df_new['Fwd_Bwd_Packet_Ratio'] = df['Total Fwd Packets'] / (df['Total Backward Packets'] + 1)\n",
    "    \n",
    "    if 'Total Length of Fwd Packets' in df.columns and 'Total Length of Bwd Packets' in df.columns:\n",
    "        df_new['Fwd_Bwd_Bytes_Ratio'] = df['Total Length of Fwd Packets'] / (df['Total Length of Bwd Packets'] + 1)\n",
    "    \n",
    "    # Flag ratios\n",
    "    flag_cols = [c for c in df.columns if 'Flag' in c and 'Count' in c]\n",
    "    if len(flag_cols) >= 2:\n",
    "        df_new['Flag_Ratio'] = df[flag_cols[0]] / (df[flag_cols[1]] + 1)\n",
    "    \n",
    "    # Packet size features\n",
    "    if 'Packet Length Mean' in df.columns and 'Packet Length Std' in df.columns:\n",
    "        df_new['Packet_CV'] = df['Packet Length Std'] / (df['Packet Length Mean'] + 1)\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "# Note: Additional features would need to be applied to original data before scaling\n",
    "print(\"Additional feature engineering functions defined.\")\n",
    "print(\"These can be applied during preprocessing pipeline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5685b1b3",
   "metadata": {},
   "source": [
    "## 7. Save Processed Data and Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b6c7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "print(\"Saving processed data...\")\n",
    "\n",
    "# Convert to DataFrame if needed\n",
    "if not isinstance(X_train_balanced, pd.DataFrame):\n",
    "    X_train_balanced = pd.DataFrame(X_train_balanced, columns=X_scaled.columns)\n",
    "\n",
    "# Save training data\n",
    "X_train_balanced.to_csv(PROCESSED_DIR / 'X_train.csv', index=False)\n",
    "pd.Series(y_train_balanced, name='label').to_csv(PROCESSED_DIR / 'y_train.csv', index=False)\n",
    "\n",
    "# Save test data\n",
    "X_test.to_csv(PROCESSED_DIR / 'X_test.csv', index=False)\n",
    "pd.Series(y_test, name='label').to_csv(PROCESSED_DIR / 'y_test.csv', index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Training data saved: {X_train_balanced.shape}\")\n",
    "print(f\"‚úÖ Test data saved: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d723ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessing artifacts\n",
    "print(\"\\nSaving preprocessing artifacts...\")\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, PROCESSED_DIR / 'scaler.pkl')\n",
    "print(\"‚úÖ Scaler saved\")\n",
    "\n",
    "# Save label encoder\n",
    "joblib.dump(le, PROCESSED_DIR / 'label_encoder.pkl')\n",
    "print(\"‚úÖ Label encoder saved\")\n",
    "\n",
    "# Save selected features list\n",
    "with open(PROCESSED_DIR / 'selected_features.txt', 'w') as f:\n",
    "    f.write('\\n'.join(X_scaled.columns))\n",
    "print(\"‚úÖ Selected features list saved\")\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance.to_csv(PROCESSED_DIR / 'feature_importance.csv', index=False)\n",
    "print(\"‚úÖ Feature importance saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1ac76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Original features: {X.shape[1]}\")\n",
    "print(f\"üìä Selected features: {X_scaled.shape[1]}\")\n",
    "print(f\"\\nüìÅ Training samples: {len(X_train_balanced)}\")\n",
    "print(f\"üìÅ Test samples: {len(X_test)}\")\n",
    "print(f\"\\nüè∑Ô∏è Classes: {len(le.classes_)}\")\n",
    "print(f\"   {list(le.classes_)}\")\n",
    "\n",
    "print(f\"\\nüìÇ Saved files:\")\n",
    "for f in PROCESSED_DIR.glob('*'):\n",
    "    print(f\"   - {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dd1312",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Proceed to:\n",
    "1. **03_model_training.ipynb** (or model_training.ipynb) - Train ML models\n",
    "2. **04_model_evaluation.ipynb** - Evaluate model performance\n",
    "3. **05_explainability.ipynb** - SHAP analysis"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
