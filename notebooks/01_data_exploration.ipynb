{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54fd7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20e1455",
   "metadata": {},
   "source": [
    "## 1. Dataset Loading\n",
    "\n",
    "### 1.1 CICIDS2017 Dataset\n",
    "Download from: https://www.unb.ca/cic/datasets/ids-2017.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e237cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "DATA_DIR = Path('../data/datasets')\n",
    "CICIDS_DIR = DATA_DIR / 'CICIDS2017'\n",
    "UNSW_DIR = DATA_DIR / 'UNSW-NB15'\n",
    "\n",
    "# List available files\n",
    "print(\"Looking for datasets...\")\n",
    "print(f\"\\nCICIDS2017 directory: {CICIDS_DIR}\")\n",
    "print(f\"UNSW-NB15 directory: {UNSW_DIR}\")\n",
    "\n",
    "# Check if directories exist\n",
    "if CICIDS_DIR.exists():\n",
    "    print(f\"\\nCICIDS2017 files: {list(CICIDS_DIR.glob('*.csv'))}\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ CICIDS2017 directory not found. Please download the dataset.\")\n",
    "\n",
    "if UNSW_DIR.exists():\n",
    "    print(f\"\\nUNSW-NB15 files: {list(UNSW_DIR.glob('*.csv'))}\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ UNSW-NB15 directory not found. Please download the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201d03ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cicids2017(sample_size=None):\n",
    "    \"\"\"\n",
    "    Load CICIDS2017 dataset.\n",
    "    \n",
    "    Args:\n",
    "        sample_size: Number of rows to sample (None for all)\n",
    "    Returns:\n",
    "        DataFrame with combined data\n",
    "    \"\"\"\n",
    "    csv_files = list(CICIDS_DIR.glob('*.csv'))\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"No CSV files found. Creating sample data for demonstration...\")\n",
    "        return create_sample_data()\n",
    "    \n",
    "    dfs = []\n",
    "    for file in csv_files:\n",
    "        print(f\"Loading: {file.name}\")\n",
    "        df = pd.read_csv(file, low_memory=False)\n",
    "        dfs.append(df)\n",
    "    \n",
    "    combined = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    if sample_size:\n",
    "        combined = combined.sample(n=min(sample_size, len(combined)), random_state=42)\n",
    "    \n",
    "    return combined\n",
    "\n",
    "\n",
    "def create_sample_data(n_samples=10000):\n",
    "    \"\"\"\n",
    "    Create sample network traffic data for demonstration.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Attack types\n",
    "    labels = ['BENIGN'] * int(n_samples * 0.7) + \\\n",
    "             ['DoS Hulk'] * int(n_samples * 0.1) + \\\n",
    "             ['PortScan'] * int(n_samples * 0.08) + \\\n",
    "             ['DDoS'] * int(n_samples * 0.05) + \\\n",
    "             ['Bot'] * int(n_samples * 0.03) + \\\n",
    "             ['FTP-Patator'] * int(n_samples * 0.02) + \\\n",
    "             ['SSH-Patator'] * int(n_samples * 0.02)\n",
    "    \n",
    "    n = len(labels)\n",
    "    \n",
    "    data = {\n",
    "        'Flow Duration': np.random.exponential(1000000, n),\n",
    "        'Total Fwd Packets': np.random.poisson(10, n),\n",
    "        'Total Backward Packets': np.random.poisson(8, n),\n",
    "        'Total Length of Fwd Packets': np.random.exponential(500, n),\n",
    "        'Total Length of Bwd Packets': np.random.exponential(400, n),\n",
    "        'Flow Bytes/s': np.random.exponential(10000, n),\n",
    "        'Flow Packets/s': np.random.exponential(100, n),\n",
    "        'Flow IAT Mean': np.random.exponential(10000, n),\n",
    "        'Flow IAT Std': np.random.exponential(5000, n),\n",
    "        'Fwd PSH Flags': np.random.binomial(5, 0.3, n),\n",
    "        'Bwd PSH Flags': np.random.binomial(3, 0.2, n),\n",
    "        'Fwd Header Length': np.random.randint(20, 100, n),\n",
    "        'Bwd Header Length': np.random.randint(20, 80, n),\n",
    "        'Packet Length Mean': np.random.exponential(100, n),\n",
    "        'Packet Length Std': np.random.exponential(50, n),\n",
    "        'Packet Length Variance': np.random.exponential(1000, n),\n",
    "        'SYN Flag Count': np.random.binomial(3, 0.4, n),\n",
    "        'ACK Flag Count': np.random.binomial(10, 0.6, n),\n",
    "        'FIN Flag Count': np.random.binomial(2, 0.3, n),\n",
    "        'RST Flag Count': np.random.binomial(1, 0.1, n),\n",
    "        'Destination Port': np.random.choice([80, 443, 22, 21, 8080, 3389], n),\n",
    "        'Protocol': np.random.choice([6, 17, 1], n, p=[0.7, 0.25, 0.05]),\n",
    "        'Label': labels\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    np.random.shuffle(df.values)\n",
    "    \n",
    "    print(f\"Created sample data with {len(df)} records\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Load data\n",
    "df = load_cicids2017(sample_size=50000)\n",
    "print(f\"\\nDataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c785bf0",
   "metadata": {},
   "source": [
    "## 2. Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dce3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nShape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1e6:.2f} MB\")\n",
    "print(f\"\\nColumn types:\")\n",
    "print(df.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f23976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first rows\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a7b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names\n",
    "print(\"Columns:\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    print(f\"{i+1:3d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f21391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1a220d",
   "metadata": {},
   "source": [
    "## 3. Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64c157c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find label column\n",
    "label_col = None\n",
    "for col in ['Label', 'label', 'attack_cat', 'Attack']:\n",
    "    if col in df.columns:\n",
    "        label_col = col\n",
    "        break\n",
    "\n",
    "if label_col:\n",
    "    print(f\"Label column: {label_col}\")\n",
    "    print(f\"\\nClass distribution:\")\n",
    "    class_dist = df[label_col].value_counts()\n",
    "    print(class_dist)\n",
    "    \n",
    "    print(f\"\\nClass percentages:\")\n",
    "    print((class_dist / len(df) * 100).round(2))\n",
    "else:\n",
    "    print(\"No label column found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1356e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "if label_col:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Bar chart\n",
    "    class_dist = df[label_col].value_counts()\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(class_dist)))\n",
    "    \n",
    "    axes[0].barh(range(len(class_dist)), class_dist.values, color=colors)\n",
    "    axes[0].set_yticks(range(len(class_dist)))\n",
    "    axes[0].set_yticklabels(class_dist.index)\n",
    "    axes[0].set_xlabel('Count')\n",
    "    axes[0].set_title('Class Distribution (Count)')\n",
    "    \n",
    "    # Pie chart\n",
    "    axes[1].pie(class_dist.values, labels=class_dist.index, autopct='%1.1f%%',\n",
    "                colors=colors, startangle=90)\n",
    "    axes[1].set_title('Class Distribution (%)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../data/processed/class_distribution.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nâš ï¸ Note: Dataset is highly imbalanced - BENIGN class dominates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24150f8e",
   "metadata": {},
   "source": [
    "## 4. Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3db4236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric columns\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Numeric features: {len(numeric_cols)}\")\n",
    "print(f\"Categorical features: {len(categorical_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dd8691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values analysis\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Missing %': missing_pct\n",
    "}).sort_values('Missing Count', ascending=False)\n",
    "\n",
    "print(\"Missing Values:\")\n",
    "print(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "if missing_df['Missing Count'].sum() == 0:\n",
    "    print(\"âœ… No missing values found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bad14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for infinite values\n",
    "inf_counts = {}\n",
    "for col in numeric_cols:\n",
    "    inf_count = np.isinf(df[col]).sum()\n",
    "    if inf_count > 0:\n",
    "        inf_counts[col] = inf_count\n",
    "\n",
    "if inf_counts:\n",
    "    print(\"Columns with infinite values:\")\n",
    "    for col, count in inf_counts.items():\n",
    "        print(f\"  {col}: {count}\")\n",
    "else:\n",
    "    print(\"âœ… No infinite values found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df248f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distributions\n",
    "sample_features = ['Flow Duration', 'Total Fwd Packets', 'Flow Bytes/s', 'Packet Length Mean']\n",
    "sample_features = [f for f in sample_features if f in df.columns][:4]\n",
    "\n",
    "if sample_features:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, feature in enumerate(sample_features):\n",
    "        # Remove infinites and outliers for visualization\n",
    "        data = df[feature].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        q99 = data.quantile(0.99)\n",
    "        data = data[data <= q99]\n",
    "        \n",
    "        axes[idx].hist(data, bins=50, edgecolor='black', alpha=0.7)\n",
    "        axes[idx].set_xlabel(feature)\n",
    "        axes[idx].set_ylabel('Frequency')\n",
    "        axes[idx].set_title(f'Distribution: {feature}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../data/processed/feature_distributions.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530080fe",
   "metadata": {},
   "source": [
    "## 5. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cc83f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top features for correlation\n",
    "top_features = numeric_cols[:15] if len(numeric_cols) > 15 else numeric_cols\n",
    "\n",
    "# Clean data for correlation\n",
    "corr_df = df[top_features].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "correlation_matrix = corr_df.corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm',\n",
    "            center=0, square=True, linewidths=0.5)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/correlation_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6521f8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find highly correlated features\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_val = abs(correlation_matrix.iloc[i, j])\n",
    "        if corr_val > 0.9:\n",
    "            high_corr_pairs.append({\n",
    "                'Feature 1': correlation_matrix.columns[i],\n",
    "                'Feature 2': correlation_matrix.columns[j],\n",
    "                'Correlation': correlation_matrix.iloc[i, j]\n",
    "            })\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(\"Highly correlated feature pairs (|r| > 0.9):\")\n",
    "    high_corr_df = pd.DataFrame(high_corr_pairs).sort_values('Correlation', key=abs, ascending=False)\n",
    "    print(high_corr_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No highly correlated pairs found (|r| > 0.9)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc4353a",
   "metadata": {},
   "source": [
    "## 6. Attack-Specific Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f632adda",
   "metadata": {},
   "outputs": [],
   "source": [
    "if label_col:\n",
    "    # Compare feature means across attack types\n",
    "    feature_cols = [c for c in numeric_cols if c != label_col][:10]\n",
    "    \n",
    "    attack_stats = df.groupby(label_col)[feature_cols].mean()\n",
    "    \n",
    "    print(\"Mean feature values by attack type:\")\n",
    "    display(attack_stats.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e68b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "if label_col:\n",
    "    # Box plots for key features by attack type\n",
    "    key_features = ['Flow Duration', 'Total Fwd Packets', 'Flow Bytes/s']\n",
    "    key_features = [f for f in key_features if f in df.columns][:3]\n",
    "    \n",
    "    if key_features:\n",
    "        fig, axes = plt.subplots(1, len(key_features), figsize=(15, 5))\n",
    "        if len(key_features) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for idx, feature in enumerate(key_features):\n",
    "            # Clean and cap outliers\n",
    "            plot_df = df[[label_col, feature]].copy()\n",
    "            plot_df = plot_df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "            q95 = plot_df[feature].quantile(0.95)\n",
    "            plot_df = plot_df[plot_df[feature] <= q95]\n",
    "            \n",
    "            plot_df.boxplot(column=feature, by=label_col, ax=axes[idx])\n",
    "            axes[idx].set_title(f'{feature} by Attack Type')\n",
    "            axes[idx].set_xlabel('Attack Type')\n",
    "            plt.sca(axes[idx])\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        plt.suptitle('')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('../data/processed/features_by_attack.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e507c8ab",
   "metadata": {},
   "source": [
    "## 7. Port Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb4d366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze destination ports\n",
    "port_col = None\n",
    "for col in ['Destination Port', 'dst_port', 'Dst Port']:\n",
    "    if col in df.columns:\n",
    "        port_col = col\n",
    "        break\n",
    "\n",
    "if port_col:\n",
    "    # Top ports\n",
    "    top_ports = df[port_col].value_counts().head(20)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    top_ports.plot(kind='bar', color='steelblue', edgecolor='black')\n",
    "    plt.xlabel('Port')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Top 20 Destination Ports')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../data/processed/top_ports.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Port distribution by attack type\n",
    "    if label_col:\n",
    "        print(\"\\nPort distribution by attack type:\")\n",
    "        port_attack = df.groupby([label_col, port_col]).size().unstack(fill_value=0)\n",
    "        print(port_attack.iloc[:, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c93736",
   "metadata": {},
   "source": [
    "## 8. Protocol Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a50044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze protocols\n",
    "proto_col = None\n",
    "for col in ['Protocol', 'protocol', 'Proto']:\n",
    "    if col in df.columns:\n",
    "        proto_col = col\n",
    "        break\n",
    "\n",
    "if proto_col:\n",
    "    # Map protocol numbers\n",
    "    proto_map = {6: 'TCP', 17: 'UDP', 1: 'ICMP'}\n",
    "    df['Protocol_Name'] = df[proto_col].map(proto_map).fillna('Other')\n",
    "    \n",
    "    proto_dist = df['Protocol_Name'].value_counts()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Bar chart\n",
    "    proto_dist.plot(kind='bar', ax=axes[0], color=['#2ecc71', '#3498db', '#e74c3c', '#95a5a6'])\n",
    "    axes[0].set_xlabel('Protocol')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_title('Protocol Distribution')\n",
    "    axes[0].tick_params(axis='x', rotation=0)\n",
    "    \n",
    "    # Protocol by attack\n",
    "    if label_col:\n",
    "        proto_attack = df.groupby([label_col, 'Protocol_Name']).size().unstack(fill_value=0)\n",
    "        proto_attack.plot(kind='bar', stacked=True, ax=axes[1])\n",
    "        axes[1].set_xlabel('Attack Type')\n",
    "        axes[1].set_ylabel('Count')\n",
    "        axes[1].set_title('Protocol by Attack Type')\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "        axes[1].legend(title='Protocol')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../data/processed/protocol_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1dd862",
   "metadata": {},
   "source": [
    "## 9. Data Quality Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a12ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary report\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA QUALITY SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nðŸ“Š Dataset Size: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
    "print(f\"ðŸ“ Memory Usage: {df.memory_usage(deep=True).sum() / 1e6:.2f} MB\")\n",
    "\n",
    "print(f\"\\nâœ… Numeric Features: {len(numeric_cols)}\")\n",
    "print(f\"âœ… Categorical Features: {len(categorical_cols)}\")\n",
    "\n",
    "missing_total = df.isnull().sum().sum()\n",
    "print(f\"\\n{'âœ…' if missing_total == 0 else 'âš ï¸'} Missing Values: {missing_total}\")\n",
    "\n",
    "inf_total = sum(np.isinf(df[col]).sum() for col in numeric_cols if df[col].dtype in ['float64', 'float32'])\n",
    "print(f\"{'âœ…' if inf_total == 0 else 'âš ï¸'} Infinite Values: {inf_total}\")\n",
    "\n",
    "if label_col:\n",
    "    n_classes = df[label_col].nunique()\n",
    "    print(f\"\\nðŸ·ï¸ Target Classes: {n_classes}\")\n",
    "    \n",
    "    # Class imbalance ratio\n",
    "    class_dist = df[label_col].value_counts()\n",
    "    imbalance_ratio = class_dist.max() / class_dist.min()\n",
    "    print(f\"âš–ï¸ Class Imbalance Ratio: {imbalance_ratio:.2f}x\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n1. Handle class imbalance (SMOTE, class weights, undersampling)\")\n",
    "print(\"2. Remove highly correlated features (>0.95)\")\n",
    "print(\"3. Scale features before ML training\")\n",
    "print(\"4. Consider log transformation for skewed features\")\n",
    "print(\"5. Replace infinite values with NaN and impute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e032da7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save exploration results\n",
    "exploration_summary = {\n",
    "    'n_samples': len(df),\n",
    "    'n_features': len(df.columns),\n",
    "    'numeric_features': len(numeric_cols),\n",
    "    'categorical_features': len(categorical_cols),\n",
    "    'missing_values': int(missing_total),\n",
    "    'infinite_values': int(inf_total),\n",
    "    'n_classes': int(df[label_col].nunique()) if label_col else 0,\n",
    "    'class_distribution': df[label_col].value_counts().to_dict() if label_col else {}\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../data/processed/exploration_summary.json', 'w') as f:\n",
    "    json.dump(exploration_summary, f, indent=2)\n",
    "\n",
    "print(\"âœ… Exploration summary saved to data/processed/exploration_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52be39e4",
   "metadata": {},
   "source": [
    "## 10. Next Steps\n",
    "\n",
    "Proceed to the next notebooks:\n",
    "1. **02_feature_engineering.ipynb** - Feature selection and transformation\n",
    "2. **03_model_training.ipynb** - Train ML models\n",
    "3. **04_model_evaluation.ipynb** - Evaluate model performance\n",
    "4. **05_explainability.ipynb** - SHAP analysis"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
