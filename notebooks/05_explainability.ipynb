{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e319c971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Try to import SHAP\n",
    "try:\n",
    "    import shap\n",
    "    shap.initjs()\n",
    "    print(f\"‚úÖ SHAP version: {shap.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå SHAP not installed. Install with: pip install shap\")\n",
    "    shap = None\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d234269a",
   "metadata": {},
   "source": [
    "## 1. Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11b1165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "DATA_DIR = Path('../data')\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "MODELS_DIR = Path('../models')\n",
    "RESULTS_DIR = DATA_DIR / 'results'\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6282be12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_data_and_model(n_samples=5000):\n",
    "    \"\"\"\n",
    "    Create sample data and train a simple model for demonstration.\n",
    "    \"\"\"\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Feature names\n",
    "    feature_names = [\n",
    "        'Flow Duration', 'Total Fwd Packets', 'Total Bwd Packets',\n",
    "        'Fwd Packet Length Mean', 'Bwd Packet Length Mean',\n",
    "        'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean',\n",
    "        'Fwd IAT Mean', 'Bwd IAT Mean', 'SYN Flag Count',\n",
    "        'ACK Flag Count', 'FIN Flag Count', 'RST Flag Count',\n",
    "        'Packet Length Mean', 'Packet Length Std', 'Packet Length Var',\n",
    "        'Destination Port', 'Init Win Bytes Fwd', 'Init Win Bytes Bwd'\n",
    "    ]\n",
    "    \n",
    "    # Generate features\n",
    "    X = np.random.randn(n_samples, len(feature_names))\n",
    "    X = pd.DataFrame(X, columns=feature_names)\n",
    "    \n",
    "    # Generate labels with correlation to features\n",
    "    score = (X['SYN Flag Count'] * 0.3 + \n",
    "             X['Flow Packets/s'] * 0.2 + \n",
    "             X['RST Flag Count'] * 0.2 +\n",
    "             np.random.randn(n_samples) * 0.3)\n",
    "    \n",
    "    labels = pd.cut(score, bins=[-np.inf, -0.5, 0.5, 1.0, np.inf],\n",
    "                    labels=['BENIGN', 'PortScan', 'DoS', 'DDoS'])\n",
    "    \n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(labels)\n",
    "    \n",
    "    # Train model\n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    return X, y, model, le\n",
    "\n",
    "# Try to load existing model and data, or create sample\n",
    "try:\n",
    "    X_test = pd.read_csv(PROCESSED_DIR / 'X_test.csv')\n",
    "    y_test = pd.read_csv(PROCESSED_DIR / 'y_test.csv')['label']\n",
    "    model = joblib.load(MODELS_DIR / 'xgboost_model.pkl')\n",
    "    le = joblib.load(PROCESSED_DIR / 'label_encoder.pkl')\n",
    "    y_test_encoded = le.transform(y_test)\n",
    "    print(\"‚úÖ Loaded existing model and data\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Creating sample data and model for demonstration...\")\n",
    "    X_test, y_test_encoded, model, le = create_sample_data_and_model()\n",
    "    y_test = pd.Series(le.inverse_transform(y_test_encoded))\n",
    "\n",
    "classes = le.classes_\n",
    "print(f\"\\nData shape: {X_test.shape}\")\n",
    "print(f\"Classes: {classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f2f6d0",
   "metadata": {},
   "source": [
    "## 2. SHAP Explainer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5902483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap:\n",
    "    # Sample data for SHAP (use subset for speed)\n",
    "    n_explain = min(1000, len(X_test))\n",
    "    X_explain = X_test.iloc[:n_explain]\n",
    "    \n",
    "    print(f\"Computing SHAP values for {n_explain} samples...\")\n",
    "    print(\"This may take a few minutes...\")\n",
    "    \n",
    "    # Create SHAP explainer\n",
    "    # Use TreeExplainer for tree-based models (faster)\n",
    "    try:\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        print(\"Using TreeExplainer\")\n",
    "    except:\n",
    "        # Fallback to KernelExplainer\n",
    "        background = shap.sample(X_test, 100)\n",
    "        explainer = shap.KernelExplainer(model.predict_proba, background)\n",
    "        print(\"Using KernelExplainer\")\n",
    "    \n",
    "    # Calculate SHAP values\n",
    "    shap_values = explainer.shap_values(X_explain)\n",
    "    \n",
    "    print(f\"\\n‚úÖ SHAP values computed!\")\n",
    "    print(f\"Shape: {np.array(shap_values).shape}\")\n",
    "else:\n",
    "    print(\"SHAP not available - skipping SHAP analysis\")\n",
    "    shap_values = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c22a4b",
   "metadata": {},
   "source": [
    "## 3. Global Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1241b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap and shap_values is not None:\n",
    "    # Summary plot - bar\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Handle multi-class SHAP values\n",
    "    if isinstance(shap_values, list):\n",
    "        # Average across classes\n",
    "        mean_shap = np.abs(np.array(shap_values)).mean(axis=0)\n",
    "        shap.summary_plot(mean_shap, X_explain, plot_type='bar', show=False)\n",
    "    else:\n",
    "        shap.summary_plot(shap_values, X_explain, plot_type='bar', show=False)\n",
    "    \n",
    "    plt.title('Global Feature Importance (SHAP)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(str(RESULTS_DIR / 'shap_global_importance.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    # Fallback to model feature importance\n",
    "    print(\"\\nUsing model's built-in feature importance:\")\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importance = pd.DataFrame({\n",
    "            'feature': X_test.columns,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=True)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.barh(importance['feature'], importance['importance'], color='steelblue')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.title('Feature Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(str(RESULTS_DIR / 'feature_importance_fallback.png'), dpi=150)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0983dcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap and shap_values is not None:\n",
    "    # Summary plot - beeswarm (shows feature value impact)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    if isinstance(shap_values, list):\n",
    "        # Use first class for beeswarm\n",
    "        shap.summary_plot(shap_values[0], X_explain, show=False)\n",
    "    else:\n",
    "        shap.summary_plot(shap_values, X_explain, show=False)\n",
    "    \n",
    "    plt.title('SHAP Values (Feature Impact on Predictions)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(str(RESULTS_DIR / 'shap_beeswarm.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6625da74",
   "metadata": {},
   "source": [
    "## 4. Per-Class Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f75d164",
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap and shap_values is not None and isinstance(shap_values, list):\n",
    "    # Feature importance per class\n",
    "    n_classes = len(classes)\n",
    "    n_cols = min(2, n_classes)\n",
    "    n_rows = (n_classes + 1) // 2\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 5*n_rows))\n",
    "    if n_classes == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for idx, (class_name, class_shap) in enumerate(zip(classes, shap_values)):\n",
    "        if idx < len(axes):\n",
    "            # Get mean absolute SHAP values\n",
    "            mean_abs_shap = np.abs(class_shap).mean(axis=0)\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'feature': X_explain.columns,\n",
    "                'importance': mean_abs_shap\n",
    "            }).sort_values('importance', ascending=True).tail(15)\n",
    "            \n",
    "            axes[idx].barh(feature_importance['feature'], \n",
    "                          feature_importance['importance'],\n",
    "                          color=plt.cm.Set2(idx / n_classes))\n",
    "            axes[idx].set_xlabel('Mean |SHAP|')\n",
    "            axes[idx].set_title(f'Top Features for: {class_name}')\n",
    "    \n",
    "    # Hide unused axes\n",
    "    for idx in range(n_classes, len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(str(RESULTS_DIR / 'shap_per_class_importance.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Per-class SHAP analysis requires multi-class SHAP values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e95f5c",
   "metadata": {},
   "source": [
    "## 5. Individual Prediction Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ebc43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_prediction(idx, X, shap_values, model, classes, explainer):\n",
    "    \"\"\"\n",
    "    Generate detailed explanation for a single prediction.\n",
    "    \"\"\"\n",
    "    sample = X.iloc[idx:idx+1]\n",
    "    prediction = model.predict(sample)[0]\n",
    "    \n",
    "    # Get probabilities if available\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        proba = model.predict_proba(sample)[0]\n",
    "    else:\n",
    "        proba = None\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"PREDICTION EXPLANATION - Sample #{idx}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nüéØ Predicted Class: {classes[prediction]}\")\n",
    "    \n",
    "    if proba is not None:\n",
    "        print(f\"\\nüìä Class Probabilities:\")\n",
    "        for i, (cls, prob) in enumerate(zip(classes, proba)):\n",
    "            bar = '‚ñà' * int(prob * 30)\n",
    "            print(f\"   {cls:15s}: {prob:.3f} {bar}\")\n",
    "    \n",
    "    return sample, prediction, proba\n",
    "\n",
    "# Explain a few predictions\n",
    "if shap and shap_values is not None:\n",
    "    # Find interesting samples (one from each class)\n",
    "    predictions = model.predict(X_explain)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SAMPLE PREDICTION EXPLANATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for class_idx, class_name in enumerate(classes[:3]):  # Limit to 3 classes\n",
    "        class_samples = np.where(predictions == class_idx)[0]\n",
    "        if len(class_samples) > 0:\n",
    "            sample_idx = class_samples[0]\n",
    "            sample, pred, proba = explain_prediction(\n",
    "                sample_idx, X_explain, shap_values, model, classes, explainer\n",
    "            )\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fba5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap and shap_values is not None:\n",
    "    # Waterfall plot for individual prediction\n",
    "    sample_idx = 0\n",
    "    \n",
    "    print(f\"\\nWaterfall plot for sample #{sample_idx}:\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Get SHAP values for this sample\n",
    "    if isinstance(shap_values, list):\n",
    "        pred_class = model.predict(X_explain.iloc[sample_idx:sample_idx+1])[0]\n",
    "        sample_shap = shap_values[pred_class][sample_idx]\n",
    "        base_value = explainer.expected_value[pred_class] if hasattr(explainer.expected_value, '__len__') else explainer.expected_value\n",
    "    else:\n",
    "        sample_shap = shap_values[sample_idx]\n",
    "        base_value = explainer.expected_value\n",
    "    \n",
    "    # Create explanation object\n",
    "    explanation = shap.Explanation(\n",
    "        values=sample_shap,\n",
    "        base_values=base_value,\n",
    "        data=X_explain.iloc[sample_idx].values,\n",
    "        feature_names=X_explain.columns.tolist()\n",
    "    )\n",
    "    \n",
    "    shap.plots.waterfall(explanation, show=False)\n",
    "    plt.title(f'SHAP Waterfall - Sample #{sample_idx}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(str(RESULTS_DIR / 'shap_waterfall.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e469c21",
   "metadata": {},
   "source": [
    "## 6. Feature Interaction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b2f799",
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap and shap_values is not None:\n",
    "    # Dependence plots for top features\n",
    "    print(\"Feature Dependence Analysis:\")\n",
    "    \n",
    "    # Get top features by importance\n",
    "    if isinstance(shap_values, list):\n",
    "        mean_shap = np.abs(np.array(shap_values)).mean(axis=(0, 1))\n",
    "    else:\n",
    "        mean_shap = np.abs(shap_values).mean(axis=0)\n",
    "    \n",
    "    top_features_idx = np.argsort(mean_shap)[-4:][::-1]  # Top 4\n",
    "    top_features = X_explain.columns[top_features_idx]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, feature in enumerate(top_features):\n",
    "        plt.sca(axes[idx])\n",
    "        \n",
    "        if isinstance(shap_values, list):\n",
    "            # Use first class for dependence plot\n",
    "            feature_idx = X_explain.columns.get_loc(feature)\n",
    "            shap.dependence_plot(\n",
    "                feature_idx, shap_values[0], X_explain,\n",
    "                ax=axes[idx], show=False\n",
    "            )\n",
    "        else:\n",
    "            shap.dependence_plot(\n",
    "                feature, shap_values, X_explain,\n",
    "                ax=axes[idx], show=False\n",
    "            )\n",
    "        \n",
    "        axes[idx].set_title(f'Dependence: {feature}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(str(RESULTS_DIR / 'shap_dependence.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10d5979",
   "metadata": {},
   "source": [
    "## 7. Attack Type Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a5227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_attack_type(attack_type, X, shap_values, model, classes, top_n=10):\n",
    "    \"\"\"\n",
    "    Generate explanation for a specific attack type.\n",
    "    \"\"\"\n",
    "    attack_idx = np.where(classes == attack_type)[0][0]\n",
    "    predictions = model.predict(X)\n",
    "    attack_samples = predictions == attack_idx\n",
    "    \n",
    "    if not attack_samples.any():\n",
    "        print(f\"No samples predicted as {attack_type}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ATTACK TYPE EXPLANATION: {attack_type}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nüìä Samples predicted as {attack_type}: {attack_samples.sum()}\")\n",
    "    \n",
    "    # Get SHAP values for attack samples\n",
    "    if isinstance(shap_values, list):\n",
    "        attack_shap = shap_values[attack_idx][attack_samples]\n",
    "    else:\n",
    "        attack_shap = shap_values[attack_samples]\n",
    "    \n",
    "    # Mean absolute SHAP values for this attack\n",
    "    mean_shap = np.abs(attack_shap).mean(axis=0)\n",
    "    \n",
    "    # Top contributing features\n",
    "    top_idx = np.argsort(mean_shap)[-top_n:][::-1]\n",
    "    \n",
    "    print(f\"\\nüîç Top {top_n} Contributing Features:\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, idx in enumerate(top_idx, 1):\n",
    "        feature = X.columns[idx]\n",
    "        importance = mean_shap[idx]\n",
    "        avg_value = X.iloc[attack_samples][feature].mean()\n",
    "        print(f\"   {i:2d}. {feature:30s} | Importance: {importance:.4f} | Avg Value: {avg_value:.2f}\")\n",
    "    \n",
    "    return mean_shap, top_idx\n",
    "\n",
    "# Explain each attack type\n",
    "if shap and shap_values is not None:\n",
    "    for attack in classes:\n",
    "        if attack != 'BENIGN':\n",
    "            try:\n",
    "                explain_attack_type(attack, X_explain, shap_values, model, classes)\n",
    "            except:\n",
    "                print(f\"Could not explain {attack}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bac5d08",
   "metadata": {},
   "source": [
    "## 8. Generate SOC-Ready Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e9801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_soc_explanation(sample, shap_vals, features, prediction, proba, classes):\n",
    "    \"\"\"\n",
    "    Generate human-readable explanation for SOC analysts.\n",
    "    \"\"\"\n",
    "    explanation = {\n",
    "        'verdict': classes[prediction],\n",
    "        'confidence': float(proba[prediction]) if proba is not None else None,\n",
    "        'risk_level': 'HIGH' if proba is not None and proba[prediction] > 0.8 else 'MEDIUM' if proba is not None and proba[prediction] > 0.5 else 'LOW',\n",
    "        'top_indicators': [],\n",
    "        'narrative': ''\n",
    "    }\n",
    "    \n",
    "    # Get top contributing features\n",
    "    top_idx = np.argsort(np.abs(shap_vals))[-5:][::-1]\n",
    "    \n",
    "    for idx in top_idx:\n",
    "        feature_name = features[idx]\n",
    "        feature_value = float(sample.iloc[0, idx])\n",
    "        shap_contribution = float(shap_vals[idx])\n",
    "        direction = 'increased' if shap_contribution > 0 else 'decreased'\n",
    "        \n",
    "        explanation['top_indicators'].append({\n",
    "            'feature': feature_name,\n",
    "            'value': feature_value,\n",
    "            'contribution': shap_contribution,\n",
    "            'direction': direction\n",
    "        })\n",
    "    \n",
    "    # Generate narrative\n",
    "    if explanation['verdict'] != 'BENIGN':\n",
    "        indicators = explanation['top_indicators'][:3]\n",
    "        narrative_parts = []\n",
    "        for ind in indicators:\n",
    "            narrative_parts.append(\n",
    "                f\"{ind['feature']} ({ind['value']:.2f}) {ind['direction']} suspicion\"\n",
    "            )\n",
    "        explanation['narrative'] = (\n",
    "            f\"This traffic was classified as {explanation['verdict']} with \"\n",
    "            f\"{explanation['confidence']*100:.1f}% confidence. \"\n",
    "            f\"Key indicators: {'; '.join(narrative_parts)}.\"\n",
    "        )\n",
    "    else:\n",
    "        explanation['narrative'] = \"Traffic appears normal with no significant anomalies detected.\"\n",
    "    \n",
    "    return explanation\n",
    "\n",
    "# Generate explanations for sample alerts\n",
    "if shap and shap_values is not None:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"SOC-READY ALERT EXPLANATIONS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    predictions = model.predict(X_explain)\n",
    "    \n",
    "    # Find attack samples\n",
    "    attack_indices = np.where(predictions != np.where(classes == 'BENIGN')[0][0] if 'BENIGN' in classes else -1)[0][:3]\n",
    "    \n",
    "    for idx in attack_indices:\n",
    "        sample = X_explain.iloc[idx:idx+1]\n",
    "        pred = predictions[idx]\n",
    "        proba = model.predict_proba(sample)[0] if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        if isinstance(shap_values, list):\n",
    "            sample_shap = shap_values[pred][idx]\n",
    "        else:\n",
    "            sample_shap = shap_values[idx]\n",
    "        \n",
    "        explanation = generate_soc_explanation(\n",
    "            sample, sample_shap, X_explain.columns.tolist(),\n",
    "            pred, proba, classes\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüö® ALERT #{idx}\")\n",
    "        print(f\"   Verdict: {explanation['verdict']}\")\n",
    "        print(f\"   Risk Level: {explanation['risk_level']}\")\n",
    "        print(f\"   Confidence: {explanation['confidence']*100:.1f}%\" if explanation['confidence'] else \"   Confidence: N/A\")\n",
    "        print(f\"\\n   üìù Narrative:\")\n",
    "        print(f\"   {explanation['narrative']}\")\n",
    "        print(f\"\\n   üîç Key Indicators:\")\n",
    "        for ind in explanation['top_indicators']:\n",
    "            print(f\"      - {ind['feature']}: {ind['value']:.3f} ({ind['direction']} risk by {abs(ind['contribution']):.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5428ee4",
   "metadata": {},
   "source": [
    "## 9. Save Explainability Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a411335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature importance rankings\n",
    "if shap and shap_values is not None:\n",
    "    if isinstance(shap_values, list):\n",
    "        mean_shap = np.abs(np.array(shap_values)).mean(axis=(0, 1))\n",
    "    else:\n",
    "        mean_shap = np.abs(shap_values).mean(axis=0)\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': X_explain.columns,\n",
    "        'mean_abs_shap': mean_shap\n",
    "    }).sort_values('mean_abs_shap', ascending=False)\n",
    "    \n",
    "    importance_df.to_csv(RESULTS_DIR / 'shap_feature_importance.csv', index=False)\n",
    "    print(f\"\\n‚úÖ Feature importance saved to: {RESULTS_DIR / 'shap_feature_importance.csv'}\")\n",
    "    \n",
    "    # Save explainability report\n",
    "    explainability_report = {\n",
    "        'analysis_date': pd.Timestamp.now().isoformat(),\n",
    "        'n_samples_analyzed': len(X_explain),\n",
    "        'n_features': len(X_explain.columns),\n",
    "        'classes': list(classes),\n",
    "        'top_10_features': importance_df.head(10).to_dict(orient='records')\n",
    "    }\n",
    "    \n",
    "    with open(RESULTS_DIR / 'explainability_report.json', 'w') as f:\n",
    "        json.dump(explainability_report, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Explainability report saved to: {RESULTS_DIR / 'explainability_report.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbd8343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXPLAINABILITY ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if shap and shap_values is not None:\n",
    "    print(f\"\\nüìä Samples analyzed: {len(X_explain)}\")\n",
    "    print(f\"üìä Features analyzed: {len(X_explain.columns)}\")\n",
    "    print(f\"\\nüèÜ Top 5 Most Important Features (SHAP):\")\n",
    "    for i, row in importance_df.head(5).iterrows():\n",
    "        print(f\"   {row['feature']}: {row['mean_abs_shap']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Results saved to: {RESULTS_DIR}\")\n",
    "    print(\"   - shap_global_importance.png\")\n",
    "    print(\"   - shap_beeswarm.png\")\n",
    "    print(\"   - shap_per_class_importance.png\")\n",
    "    print(\"   - shap_waterfall.png\")\n",
    "    print(\"   - shap_dependence.png\")\n",
    "    print(\"   - shap_feature_importance.csv\")\n",
    "    print(\"   - explainability_report.json\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è SHAP analysis was not performed.\")\n",
    "    print(\"Install SHAP with: pip install shap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a6bb58",
   "metadata": {},
   "source": [
    "## Key Insights for SOC Analysts\n",
    "\n",
    "### How to Use SHAP Explanations:\n",
    "\n",
    "1. **Global Importance**: Shows which features are most influential across all predictions\n",
    "2. **Beeswarm Plot**: Shows how feature values (high/low) affect predictions\n",
    "3. **Waterfall Plot**: Explains individual alerts step-by-step\n",
    "4. **Dependence Plots**: Reveal feature interactions\n",
    "\n",
    "### Actionable Recommendations:\n",
    "\n",
    "- Focus monitoring on top features identified by SHAP\n",
    "- Set thresholds based on feature values that increase attack probability\n",
    "- Use individual explanations to validate/triage alerts\n",
    "- Correlate SHAP insights with known attack signatures"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
