{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fce9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# AI-NIDS modules\n",
    "from ml.preprocessing.preprocessor import DataPreprocessor, FeatureEngineer\n",
    "from ml.models.xgboost_classifier import XGBoostClassifier\n",
    "from ml.models.autoencoder import AnomalyAutoencoder\n",
    "from ml.models.lstm_detector import LSTMDetector\n",
    "from ml.models.ensemble import EnsembleDetector\n",
    "from ml.explainability.shap_explainer import SHAPExplainer\n",
    "from ml.training import ModelTrainer\n",
    "\n",
    "# Settings\n",
    "plt.style.use('dark_background')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "print('✅ Libraries loaded successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94725cb3",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Exploration\n",
    "\n",
    "Load your dataset (CICIDS2017 or UNSW-NB15) and explore its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3188671d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# Option 1: CICIDS2017\n",
    "# df = pd.read_csv('../data/raw/cicids2017.csv')\n",
    "\n",
    "# Option 2: UNSW-NB15\n",
    "# df = pd.read_csv('../data/raw/unsw_nb15.csv')\n",
    "\n",
    "# For demo, create synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples = 10000\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'duration': np.random.exponential(10, n_samples),\n",
    "    'protocol_type': np.random.choice(['tcp', 'udp', 'icmp'], n_samples),\n",
    "    'src_bytes': np.random.exponential(1000, n_samples),\n",
    "    'dst_bytes': np.random.exponential(1000, n_samples),\n",
    "    'count': np.random.poisson(10, n_samples),\n",
    "    'srv_count': np.random.poisson(5, n_samples),\n",
    "    'serror_rate': np.random.beta(2, 10, n_samples),\n",
    "    'rerror_rate': np.random.beta(2, 10, n_samples),\n",
    "    'same_srv_rate': np.random.beta(8, 2, n_samples),\n",
    "    'diff_srv_rate': np.random.beta(2, 8, n_samples),\n",
    "    'label': np.random.choice([0, 1], n_samples, p=[0.9, 0.1])\n",
    "})\n",
    "\n",
    "print(f'Dataset shape: {df.shape}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb0f911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration\n",
    "print('Dataset Info:')\n",
    "print(f'  Samples: {len(df):,}')\n",
    "print(f'  Features: {len(df.columns) - 1}')\n",
    "print(f'  Target distribution:')\n",
    "print(df['label'].value_counts(normalize=True))\n",
    "\n",
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "df['label'].value_counts().plot(kind='bar', ax=axes[0], color=['#00ff88', '#ff4444'])\n",
    "axes[0].set_title('Class Distribution')\n",
    "axes[0].set_xlabel('Class')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "df['label'].value_counts().plot(kind='pie', ax=axes[1], autopct='%1.1f%%', colors=['#00ff88', '#ff4444'])\n",
    "axes[1].set_title('Class Proportion')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb333566",
   "metadata": {},
   "source": [
    "## 2. Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af69185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=['label'])\n",
    "y = df['label']\n",
    "\n",
    "# Fit and transform\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "print(f'Processed features shape: {X_processed.shape}')\n",
    "print(f'Feature names: {list(X_processed.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c0adec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_processed, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f'Training set:   {len(X_train):,} samples')\n",
    "print(f'Validation set: {len(X_val):,} samples')\n",
    "print(f'Test set:       {len(X_test):,} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9725f3",
   "metadata": {},
   "source": [
    "## 3. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914a04c4",
   "metadata": {},
   "source": [
    "### 3.1 XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e7ce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost\n",
    "xgb_model = XGBoostClassifier(\n",
    "    params={\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.1,\n",
    "        'n_estimators': 100,\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc'\n",
    "    }\n",
    ")\n",
    "\n",
    "xgb_model.train(X_train, y_train, X_val=X_val, y_val=y_val)\n",
    "print('✅ XGBoost training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c104a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate XGBoost\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "y_prob_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print('XGBoost Classification Report:')\n",
    "print(classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87724c6",
   "metadata": {},
   "source": [
    "### 3.2 Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700e7664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Autoencoder on normal traffic only\n",
    "X_train_normal = X_train[y_train == 0]\n",
    "\n",
    "autoencoder = AnomalyAutoencoder(\n",
    "    input_dim=X_train.shape[1],\n",
    "    encoding_dim=16\n",
    ")\n",
    "\n",
    "ae_history = autoencoder.train(\n",
    "    X_train_normal.values,\n",
    "    epochs=50,\n",
    "    batch_size=128\n",
    ")\n",
    "print('✅ Autoencoder training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767ac67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot autoencoder training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(ae_history['train_loss'], label='Training Loss')\n",
    "if 'val_loss' in ae_history:\n",
    "    plt.plot(ae_history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Autoencoder Training History')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a0111f",
   "metadata": {},
   "source": [
    "### 3.3 LSTM Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946d7566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sequence data for LSTM\n",
    "sequence_length = 10\n",
    "\n",
    "lstm_detector = LSTMDetector(\n",
    "    input_size=X_train.shape[1],\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    sequence_length=sequence_length\n",
    ")\n",
    "\n",
    "lstm_history = lstm_detector.train(\n",
    "    X_train.values,\n",
    "    y_train.values,\n",
    "    epochs=30,\n",
    "    batch_size=64\n",
    ")\n",
    "print('✅ LSTM training complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188d36df",
   "metadata": {},
   "source": [
    "## 4. Ensemble Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95495177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble detector\n",
    "ensemble = EnsembleDetector(model_dir='../models')\n",
    "\n",
    "# Train all models\n",
    "ensemble.train(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# Save models\n",
    "ensemble.save()\n",
    "print('✅ Ensemble training and saving complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8c1ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate ensemble\n",
    "y_pred_ensemble, confidence = ensemble.predict(X_test)\n",
    "\n",
    "print('Ensemble Classification Report:')\n",
    "print(classification_report(y_test, y_pred_ensemble))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a28270",
   "metadata": {},
   "source": [
    "## 5. Evaluation & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8102fa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "models = {\n",
    "    'XGBoost': (y_pred_xgb, y_prob_xgb),\n",
    "    'Ensemble': (y_pred_ensemble, confidence)\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, (y_pred, y_prob) in models.items():\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "        'F1': f1_score(y_test, y_pred, zero_division=0),\n",
    "        'ROC-AUC': roc_auc_score(y_test, y_prob) if y_prob is not None else None\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.set_index('Model', inplace=True)\n",
    "results_df.style.format('{:.4f}').background_gradient(cmap='Greens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa864342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for ax, (name, (y_pred, _)) in zip(axes, models.items()):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=['Normal', 'Attack'],\n",
    "                yticklabels=['Normal', 'Attack'])\n",
    "    ax.set_title(f'{name} Confusion Matrix')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf2c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for name, (_, y_prob) in models.items():\n",
    "    if y_prob is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "        auc = roc_auc_score(y_test, y_prob)\n",
    "        plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.4f})', linewidth=2)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random', alpha=0.5)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves Comparison')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511b903d",
   "metadata": {},
   "source": [
    "## 6. SHAP Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba2743a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SHAP explainer\n",
    "explainer = SHAPExplainer(xgb_model.model, X_train[:1000])\n",
    "\n",
    "print('✅ SHAP explainer initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a371ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain a single prediction\n",
    "sample_idx = 0\n",
    "sample = X_test.iloc[[sample_idx]]\n",
    "\n",
    "explanation = explainer.explain(sample)\n",
    "\n",
    "print(f'\\nPrediction: {\"Attack\" if y_pred_xgb[sample_idx] else \"Normal\"}')\n",
    "print(f'Actual: {\"Attack\" if y_test.iloc[sample_idx] else \"Normal\"}')\n",
    "print(f'\\nTop contributing features:')\n",
    "for feature, value in explanation['top_features']:\n",
    "    print(f'  {feature}: {value:+.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301016ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP summary plot\n",
    "import shap\n",
    "\n",
    "# Calculate SHAP values for test set\n",
    "shap_values = explainer.shap_values(X_test[:500])\n",
    "\n",
    "# Summary plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values, X_test[:500], show=False)\n",
    "plt.title('SHAP Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99c7811",
   "metadata": {},
   "source": [
    "## 7. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eff0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all models\n",
    "import os\n",
    "\n",
    "model_dir = '../models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save XGBoost\n",
    "xgb_model.save(f'{model_dir}/xgboost_model.json')\n",
    "\n",
    "# Save Autoencoder\n",
    "autoencoder.save(f'{model_dir}/autoencoder.pt')\n",
    "\n",
    "# Save LSTM\n",
    "lstm_detector.save(f'{model_dir}/lstm_detector.pt')\n",
    "\n",
    "# Save preprocessor\n",
    "import joblib\n",
    "joblib.dump(preprocessor, f'{model_dir}/preprocessor.pkl')\n",
    "\n",
    "print('✅ All models saved successfully')\n",
    "print(f'\\nModels saved to: {os.path.abspath(model_dir)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cc61e4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete AI-NIDS training pipeline:\n",
    "\n",
    "1. **Data Loading**: Loaded and explored the network traffic dataset\n",
    "2. **Preprocessing**: Applied feature engineering and normalization\n",
    "3. **Model Training**: Trained XGBoost, Autoencoder, and LSTM models\n",
    "4. **Ensemble**: Combined models for improved detection\n",
    "5. **Evaluation**: Compared model performance with metrics and visualizations\n",
    "6. **Explainability**: Used SHAP to understand model decisions\n",
    "\n",
    "### Next Steps\n",
    "- Replace synthetic data with real CICIDS2017 or UNSW-NB15 dataset\n",
    "- Fine-tune hyperparameters for better performance\n",
    "- Deploy models to the AI-NIDS system"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
